{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the data by performing an HTTP call to the OpenDev resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_length = 10000\n",
    "full_data = np.array([])\n",
    "for i in range(500, full_data_length + 1, 500):\n",
    "    params = {\n",
    "        'O': 81,\n",
    "        'S': 0,\n",
    "        'n': i,  \n",
    "        'q': '-status:merged'\n",
    "    }\n",
    "\n",
    "    response = requests.get(\"https://review.opendev.org/changes/\",\n",
    "                            params=params)\n",
    "\n",
    "    data_per_request = response.text.split(\"\\n\")[1]\n",
    "\n",
    "    data_per_request = np.array(json.loads(data_per_request))\n",
    "\n",
    "    full_data = np.concatenate((full_data, data_per_request))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure that the data has the same length as indicated in the url parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple function that combines the columns of the entire dataset, and excluding duplicated ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retierve_cols():\n",
    "    '''This fuction serves for extracting all columns of the data\n",
    "    \\nIt is optional and not used for the extraction process\n",
    "    '''\n",
    "    columns = np.array([])\n",
    "\n",
    "    for i in range(full_data_length):\n",
    "        # dd = dict()\n",
    "        columns = np.hstack((columns, list(full_data[i].keys())))\n",
    "\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = set(retierve_cols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The original columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'owner', 'created', '_more_changes', 'project', 'change_id', 'total_comment_count', 'labels', 'cherry_pick_of_change', 'insertions', 'requirements', 'has_review_started', 'reviewers', 'status', 'branch', 'id', 'subject', 'removable_reviewers', 'hashtags', 'submit_records', 'unresolved_comment_count', 'cherry_pick_of_patch_set', 'meta_rev_id', 'deletions', 'submit_type', 'updated', 'mergeable', 'topic', 'pending_reviewers', 'work_in_progress', '_number', 'attention_set'}\n"
     ]
    }
   ],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This utility function helps to deal with any given data scheme, be it simple or complex data structure. This way, it can easily be adapted to any future data scheme changes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_object(obj):\n",
    "\n",
    "    keys = obj.keys()\n",
    "    new_item = {}\n",
    "\n",
    "    for k in keys:\n",
    "    \n",
    "        if (type(obj[k]) is int or type(obj[k]) is str\n",
    "                or type(obj[k]) is bool):\n",
    "            new_item[k] = obj[k]\n",
    "    \n",
    "        elif type(obj[k]) is list or type(obj[k]) is tuple:\n",
    "            new_attr_name = (k + \"_count\").replace(\"__\", \"_\")\n",
    "            new_item[new_attr_name] = len(obj[k])\n",
    "    \n",
    "        elif type(obj[k]) is dict:\n",
    "            dict_obj = {}\n",
    "           \n",
    "            for k2 in obj[k].keys():\n",
    "                new_dict_attr = (k+\"_\"+k2).replace(\"__\", \"_\")\n",
    "                dict_obj[new_dict_attr] = obj[k][k2]\n",
    "           \n",
    "            return build_object(new_item | dict_obj)\n",
    "    \n",
    "    return new_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function is designed to transform the original dataset to a well-structured one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    clean_data = []\n",
    "\n",
    "    for d in data:\n",
    "\n",
    "        new_obj = build_object(d)\n",
    "\n",
    "        clean_data.append(new_obj)\n",
    "\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = process_data(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the newly-transformed dataset to pandas for further processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path('./openstack_data.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_csv(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
