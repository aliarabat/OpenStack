{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import utils.helpers as hpr\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_openstack_data():\n",
    "    '''Combine generated csv files into a single DataFrame object\n",
    "    '''\n",
    "    df = pd.DataFrame([])\n",
    "    data_path = \"%sChanges/\" % hpr.DIR\n",
    "    changes_file_names = hpr.list_file(data_path)\n",
    "    for f in changes_file_names:\n",
    "        df_per_file = pd.read_csv(\"%s%s\" % (data_path, f))\n",
    "        df = pd.concat((df, df_per_file))\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"number\"])\n",
    "\n",
    "    df = df.sort_values(by=\"updated\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combine_openstack_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### link service to projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_repositories = pd.read_csv(\"./all_os_components.csv\")\n",
    "\n",
    "online_repositories[\"related_projects\"] = online_repositories[\"related_projects\"].apply(ast.literal_eval)\n",
    "\n",
    "online_repositories = dict(zip(online_repositories[\"main_project\"].values.reshape(-1), online_repositories[\"related_projects\"].values))\n",
    "\n",
    "switched_key_values = {\"openstack/%s\" % item: key for key, value in online_repositories.items() for item in value}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure the duration and the number of revisions a developer need to work out until a dependency is identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(start, end):\n",
    "    if start > end:\n",
    "        start, end = end, start\n",
    "    current_date =  datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\") \n",
    "    previous_date = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\") \n",
    "    diff = current_date - previous_date\n",
    "    diff = float(\"{:.2f}\".format(diff.total_seconds() / 3600))\n",
    "    return diff\n",
    "\n",
    "\n",
    "def extract_attr(x, attr):\n",
    "    '''Extracts the passed-on parameter values out of the commit message \n",
    "    '''\n",
    "    rs = re.findall(\"%s:\\s[a-zA-Z0-9/\\.\\:\\+\\-\\#]{6,}\" % (attr), x)\n",
    "    result = []\n",
    "    for row in rs:\n",
    "        row = row[len(attr) + 2:]\n",
    "        change_id_pattern = re.search(r\"[a-zA-Z0-9]{41}\", row)\n",
    "        if change_id_pattern:\n",
    "            result.append(change_id_pattern[0])\n",
    "            continue\n",
    "        number_pattern = re.search(\"#?https?[\\:][/]{2}review[\\.](opendev|openstack)[\\.]org([a-z0-9A-Z\\-\\+/\\.#]*)\\d+\", row)\n",
    "        if number_pattern:\n",
    "            result.append(int(re.search(\"\\d+$\", number_pattern[0][0:])[0]))\n",
    "    return result if len(result) != 0 else None\n",
    "\n",
    "\n",
    "def retrieve_revision_date(row, attr, return_revision_date=True):\n",
    "    number = None\n",
    "    second_number = None\n",
    "\n",
    "    if attr == \"Depends-On\":\n",
    "        number = row[\"Target\"]\n",
    "        second_number = row[\"Source\"]\n",
    "        change_id = row[\"Source_change_id\"]\n",
    "    else:\n",
    "        number = row[\"Source\"]\n",
    "        second_number = row[\"Target\"]\n",
    "        change_id = row[\"Target_change_id\"]\n",
    "\n",
    "    df_row = df.loc[df[\"number\"] == number]\n",
    "    revisions = ast.literal_eval(df_row[\"revisions\"].values[0])\n",
    "    revisions = sorted(revisions, key=lambda x: x[\"created\"])\n",
    "    if  len(revisions) == 1:\n",
    "        if return_revision_date:\n",
    "            return revisions[0][\"created\"]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    first_revision = revisions[0]\n",
    "    first_message = first_revision[\"message\"]\n",
    "\n",
    "    results = extract_attr(first_message, attr)\n",
    "\n",
    "    if results and ((change_id in results) or (second_number in results)):\n",
    "        if return_revision_date:\n",
    "            return first_revision[\"created\"]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    for i in range(1,len(revisions)):\n",
    "        current_message = revisions[i][\"message\"]\n",
    "        created = revisions[i][\"created\"]\n",
    "        results = extract_attr(current_message, attr)\n",
    "        \n",
    "        if results and ((change_id in results) or (second_number in results)):\n",
    "\n",
    "            if return_revision_date:\n",
    "                return created\n",
    "            else:\n",
    "                return i + 1\n",
    "\n",
    "def is_same_developer(row):\n",
    "    return \"Same\" if row[\"Source_dev\"] == row[\"Target_dev\"] else \"Different\"\n",
    "\n",
    "def identify_dependency(row):\n",
    "    source_date = row[\"Source_date\"] \n",
    "    target_date = row[\"Target_date\"]\n",
    "    link_date = row[\"link_date\"]\n",
    "\n",
    "    return min(time_diff(target_date[:-10], link_date[:-10]), time_diff(source_date[:-10], link_date[:-10]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_repositories = pd.read_csv(\"./all_os_components.csv\")\n",
    "\n",
    "online_repositories[\"related_projects\"] = online_repositories[\"related_projects\"].apply(ast.literal_eval)\n",
    "\n",
    "online_repositories = dict(zip(online_repositories[\"main_project\"].values.reshape(-1), online_repositories[\"related_projects\"].values))\n",
    "\n",
    "switched_key_values = {\"openstack/%s\" % item: key for key, value in online_repositories.items() for item in value}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depends-On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depends_on = pd.read_csv(\"./Files/source_target_depends.csv\")\n",
    "\n",
    "df_depends_on[\"Source_status\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"status\"].values[0])\n",
    "df_depends_on[\"Target_status\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"status\"].values[0])\n",
    "\n",
    "df_depends_on[\"revisions\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"revisions\"].values[0])\n",
    "df_depends_on[\"Source_change_id\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"change_id\"].values[0])\n",
    "df_depends_on[\"Target_change_id\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"change_id\"].values[0])\n",
    "\n",
    "df_depends_on[\"link_date\"] = df_depends_on.apply(retrieve_revision_date, args=(\"Depends-On\",), axis=1)\n",
    "\n",
    "df_depends_on[\"worked_revisions\"] = df_depends_on.apply(retrieve_revision_date, args=(\"Depends-On\",False,), axis=1)\n",
    "\n",
    "df_depends_on[\"is_cross\"] = df_depends_on.apply(lambda row: \"Cross\" if row[\"Source_repo\"]!=row[\"Target_repo\"] else \"Same\", axis=1)\n",
    "\n",
    "df_depends_on[\"is_source_bot\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x,\"is_owner_bot\"].values[0])\n",
    "df_depends_on[\"is_target_bot\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x,\"is_owner_bot\"].values[0])\n",
    "\n",
    "df_depends_on[\"Source_dev\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x,\"owner_account_id\"].values[0])\n",
    "df_depends_on[\"Target_dev\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x,\"owner_account_id\"].values[0])\n",
    "\n",
    "df_depends_on[\"same_dev\"] = df_depends_on.apply(is_same_developer, axis=1)\n",
    "\n",
    "df_depends_on[\"Source_service\"] = df_depends_on[\"Source_repo\"].map(lambda x: switched_key_values.get(x))\n",
    "df_depends_on[\"Target_service\"] = df_depends_on[\"Target_repo\"].map(lambda x: switched_key_values.get(x))\n",
    "\n",
    "df_depends_on[\"Source_date\"] = df_depends_on[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"created\"].values[0])\n",
    "df_depends_on[\"Target_date\"] = df_depends_on[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"created\"].values[0])\n",
    "\n",
    "df_depends_on[\"when_identified\"] = df_depends_on[[\"Source_date\", \"Target_date\", \"link_date\"]].apply(identify_dependency, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed-By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed_by = pd.read_csv(\"./Files/source_target_needed.csv\")\n",
    "\n",
    "df_needed_by[\"Source_status\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"status\"].values[0])\n",
    "df_needed_by[\"Target_status\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"status\"].values[0])\n",
    "\n",
    "df_needed_by[\"revisions\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"revisions\"].values[0])\n",
    "df_needed_by[\"Source_change_id\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"change_id\"].values[0])\n",
    "df_needed_by[\"Target_change_id\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"change_id\"].values[0])\n",
    "\n",
    "df_needed_by[\"link_date\"] = df_needed_by.apply(retrieve_revision_date, args=(\"Needed-By\",), axis=1)\n",
    "df_needed_by[\"worked_revisions\"] = df_needed_by.apply(retrieve_revision_date, args=(\"Needed-By\",False,), axis=1)\n",
    "\n",
    "df_needed_by[\"is_cross\"] = df_needed_by.apply(lambda row: \"Cross\" if row[\"Source_repo\"]!=row[\"Target_repo\"] else \"Same\", axis=1)\n",
    "\n",
    "df_needed_by[\"is_source_bot\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x,\"is_owner_bot\"].values[0])\n",
    "df_needed_by[\"is_target_bot\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x,\"is_owner_bot\"].values[0])\n",
    "\n",
    "df_needed_by[\"Source_dev\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x,\"owner_account_id\"].values[0])\n",
    "df_needed_by[\"Target_dev\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x,\"owner_account_id\"].values[0])\n",
    "\n",
    "df_needed_by[\"same_dev\"] = df_needed_by.apply(is_same_developer, axis=1)\n",
    "\n",
    "df_needed_by[\"Source_service\"] = df_needed_by[\"Source_repo\"].map(lambda x: switched_key_values.get(x))\n",
    "df_needed_by[\"Target_service\"] = df_needed_by[\"Target_repo\"].map(lambda x: switched_key_values.get(x))\n",
    "\n",
    "df_needed_by[\"Source_date\"] = df_needed_by[\"Source\"].map(lambda x: df.loc[df[\"number\"]==x, \"created\"].values[0])\n",
    "df_needed_by[\"Target_date\"] = df_needed_by[\"Target\"].map(lambda x: df.loc[df[\"number\"]==x, \"created\"].values[0])\n",
    "\n",
    "df_needed_by[\"when_identified\"] = df_needed_by[[\"Source_date\", \"Target_date\", \"link_date\"]].apply(identify_dependency, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of Depends-On and Needed-By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_identification = pd.concat((df_depends_on, def_needed_by)).sort_values(\"when_identified\")\n",
    "dependency_identification = dependency_identification.drop_duplicates(subset=[\"Source\", \"Target\"], keep=\"First\")\n",
    "\n",
    "dependency_identification.to_csv(\"./RQs/RQ5/Files/dependency_identification.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lags of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_dependencies = pd.read_csv(\"./Files/all_dependencies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_dependencies.loc[(df_all_dependencies[\"Source_status\"] == \"MERGED\") &\n",
    "                      (df_all_dependencies[\"Target_status\"] == \"MERGED\") &\n",
    "                      (df_all_dependencies[\"is_source_bot\"] == False) & \n",
    "                      (df_all_dependencies[\"is_target_bot\"] == False),\n",
    "                      [\"Source\", \"Target\", \"Source_repo\", \"Target_repo\", \"is_cross\", \"is_cross_service\", \"Source_service\", \"Target_service\", \"lag\", \"is_same_dev\"]].to_csv(\"./RQs/RQ5/Files/all_lags.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lags = pd.read_csv(\"./RQs/RQ5/Files/all_lags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/wn9d12jj6g78td0tl7gx7xlw0000gp/T/ipykernel_1637/2418867077.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  all_lags.loc[(all_lags.is_same_dev==\"Same\")&(all_lags.is_cross_service==\"Cross\")].median()#sort_values(\"lag\").iloc[5600:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Source    467725.00\n",
       "Target    467972.00\n",
       "lag            1.43\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lags.loc[(all_lags.is_same_dev==\"Same\")&(all_lags.is_cross_service==\"Cross\")].median()#sort_values(\"lag\").iloc[5600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3252418944152096"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lags.loc[(all_lags.lag<=1)&(all_lags.is_cross==\"Cross\"), [\"lag\"]])/len(all_lags.loc[(all_lags.is_cross==\"Cross\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lags.loc[(all_lags[\"Source_status\"] == \"MERGED\") &\n",
    "                      (all_lags[\"Target_status\"] == \"MERGED\") &\n",
    "                      (all_lags[\"is_source_bot\"] == False) &\n",
    "                      (all_lags[\"is_target_bot\"] == False) &\n",
    "                      (all_lags[\"is_cross_service\"] == \"Cross\") &\n",
    "                      (all_lags[\"same_dev\"] == \"Same\") &\n",
    "                      (all_lags[\"lag\"] > 0), [\n",
    "                          \"Source\", \"Target\", \"Source_repo\", \"Target_repo\", \"is_cross\", \"same_dev\",\n",
    "                          \"lag\", \"Source_status\", \"Target_status\"\n",
    "                      ]].median()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lag of chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_paths_number = pd.read_csv(\"./Files/Number/extended_merged.csv\")\n",
    "\n",
    "extended_paths_number[\"Path\"] = extended_paths_number[\"Path\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_paths_number[\"Path_merged\"] = extended_paths_number[\"Path\"].map(lambda path: df.loc[df[\"number\"].isin(path)&(df[\"status\"]==\"MERGED\"), \"number\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_lists(list1):\n",
    "#     merged = []\n",
    "#     while len(list1) > 0:\n",
    "#         current_list = list1.pop(0)\n",
    "#         merged_list = current_list.copy()\n",
    "#         i = 0\n",
    "#         while i < len(list1):\n",
    "#             if any(elem in list1[i] for elem in current_list):\n",
    "#                 merged_list.extend(list1.pop(i))\n",
    "#             else:\n",
    "#                 i += 1\n",
    "#         merged.append(list(set(merged_list)))\n",
    "#     return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_paths_number[\"length_merged\"] = extended_paths_number[\"Path_merged\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_paths_number[\"is_cross\"] = extended_paths_number.loc[extended_paths_number[\"length_merged\"]>1, \"Path_merged\"].map(lambda path: True if df.loc[df[\"number\"].isin(path), \"project\"].nunique() > 1 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_paths_number_merged = extended_paths_number[extended_paths_number[\"length_merged\"]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chain_lag(path):\n",
    "    dates = df.loc[df[\"number\"].isin(path), [\"project\", \"created\"]].sort_values(\"created\")\n",
    "    dates = dates[\"created\"].values\n",
    "    start_date = dates[0][:-10]\n",
    "    end_date = dates[-1][:-10]\n",
    "    return time_diff(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/wn9d12jj6g78td0tl7gx7xlw0000gp/T/ipykernel_79732/95147171.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  extended_paths_number_merged[\"lag\"] = extended_paths_number_merged[\"Path_merged\"].map(compute_chain_lag)\n"
     ]
    }
   ],
   "source": [
    "extended_paths_number_merged[\"lag\"] = extended_paths_number_merged[\"Path_merged\"].map(compute_chain_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chain_cross_service(path):\n",
    "    services = [switched_key_values.get(p) for p in df.loc[df[\"number\"].isin(path), \"project\"].unique()]\n",
    "    if len(set(services)) > 1:\n",
    "        return \"Cross\"\n",
    "    elif len(set(services)) == 1:\n",
    "        return \"Same\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_paths_number_merged[\"is_cross_service\"] = extended_paths_number_merged[\"Path_merged\"].map(is_chain_cross_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_paths_number_merged.to_csv(\"./RQs/RQ5/Files/chains_lag.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/wn9d12jj6g78td0tl7gx7xlw0000gp/T/ipykernel_79732/1086663655.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  extended_paths_number_merged[(extended_paths_number_merged.is_cross_service==True)&(extended_paths_number_merged.lag>0)].median()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "length               3.00\n",
       "all_abandoned        0.00\n",
       "is_cross             1.00\n",
       "lag                 90.46\n",
       "is_cross_service     1.00\n",
       "length_merged        2.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_paths_number_merged[(extended_paths_number_merged.is_cross_service==True)&(extended_paths_number_merged.lag>0)].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_depends_needed.to_csv(\"./RQs/RQ5/Files/dependency_identification.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019747520288548242"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extended_paths_number_merged[(extended_paths_number_merged.is_cross==\"Cross\")&(extended_paths_number_merged.lag==0)])/len(extended_paths_number_merged[(extended_paths_number_merged.is_cross==\"Cross\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of when developers find a dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_identification = pd.read_csv(\"./RQs/RQ5/Files/dependency_identification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_identification[\"same_dev\"] = dependency_identification[\"same_dev\"].map(lambda x: \"Different\" if x == False else \"Same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/wn9d12jj6g78td0tl7gx7xlw0000gp/T/ipykernel_1504/432506136.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_depends_needed[(df_depends_needed[\"is_cross_service\"]==\"Cross\")].median()#sort_values(\"when_identified\")#.iloc[19000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Source              573094.0\n",
       "Target              573135.0\n",
       "worked_revisions         1.0\n",
       "is_cross                 1.0\n",
       "is_source_bot            0.0\n",
       "is_target_bot            0.0\n",
       "Source_dev           10135.0\n",
       "Target_dev           10135.0\n",
       "same_dev                 1.0\n",
       "when_identified          0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_identification[(dependency_identification[\"is_cross_service\"]==\"Cross\")].median()#sort_values(\"when_identified\")#.iloc[19000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c3f98dc7a618963d8c453529bd2f978a19750ea04392a0abd6ac6b1e473063f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
